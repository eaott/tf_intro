{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation in TensorFlow\n",
    "In the presentation, we had an interesting discussion about how differentiation\n",
    "works within TensorFlow. `tf.gradients` says it uses \"symbolic differentiation,\"\n",
    "which is what I (Evan) said. Most computing-based differentiation strategies\n",
    "do automatic or numerical differentiation, so the question is what TensorFlow *actually* uses.\n",
    "\n",
    "The TensorFlow code (and documentation!) seems to suggest it's attempting to do \n",
    "symbolic differentiation (see\n",
    "[the TensorFlow GitHub repo](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/gradients_impl.py)\n",
    "-- specifically looking for `symbolic_gradient`).\n",
    "\n",
    "[Numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation)\n",
    "is the process of adding small noise to the input of a function and monitoring\n",
    "the change in the function. That doesn't seem to be anywhere in TensorFlow.\n",
    "\n",
    "The definitions of symbolic and automatic differentiation seem to be somewhat blurry depending on where you look. [This blog](https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/) about automatic differentiation seems to suggest that symbolic differentiation is purely mechanical, with a set of known rules and little-to-no simplification. Automatic differentiation seems to be smarter.\n",
    "\n",
    "TensorFlow seems more of a simple symbolic differentiation system, in that it has many rules for derivatives, that are applied via the chain rule. For example (from [this part of the codebase](https://github.com/tensorflow/tensorflow/blob/b474e55c23e5cc42b01a1ddea34751f01110deb6/tensorflow/python/ops/math_grad.py)):\n",
    "\n",
    "```python\n",
    "@ops.RegisterGradient(\"Abs\")\n",
    "def _AbsGrad(op, grad):\n",
    "  x = op.inputs[0]\n",
    "  return grad * math_ops.sign(x)\n",
    "```\n",
    "\n",
    "specifies that the derivative of the absolute value function is the\n",
    "sign of the input (which would seem to answer the question about the $\\ell1$ subgradient question). The `grad` term that is part of the input and multiplied\n",
    "in the output is for backpropagation -- we start at the end and work\n",
    "our way backward, updating the gradient as we go. By the time we reach\n",
    "a node applying the absolute value function on `f(x)`, we've already computed\n",
    "the chain rule for `f'(x)`.\n",
    "\n",
    "As another example, `exp(x)` is implemented as\n",
    "```python\n",
    "@ops.RegisterGradient(\"Exp\")\n",
    "def _ExpGrad(op, grad):\n",
    "  \"\"\"Returns grad * exp(x).\"\"\"\n",
    "  y = op.outputs[0]  # y = e^x\n",
    "  with ops.control_dependencies([grad]):\n",
    "    y = math_ops.conj(y)\n",
    "    return grad * y\n",
    "```\n",
    "where the gradient of $\\exp(x)$ is the output of the function times\n",
    "the derivative of everything above this node in the graph: If we compute $z = 2 \\exp(x)$, we can break this up into $y = \\exp(x)$ and $z=2y$. Then $\\frac{dz}{dy}=2$ and $\\frac{dy}{dx}=y$. So, to compute the derivative $\\frac{dz}{dx}$, we just do the chain rule (in this case, we're doing \"backpropagation\").\n",
    "\n",
    "At all points, TensorFlow just does a lookup for each derivative based on rules predefined in the language, or defined in a custom operation. By default, new operations just compose existing operations, so the derivative is well-defined without additional work. But in some cases (or to improve efficiency based on leveraging some tricks) we can in principle define new fundamental operations. If you're interested in that, check out TensorFlow documentation on [creating an op](https://www.tensorflow.org/extend/adding_an_op). For almost all cases though, you shouldn't need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacking symbolic differentiation\n",
    "Because TensorFlow more-or-less does simple symbolic differentation, we can use the computation graph it constructs to explore the form of the derivative, rather than just the value for a particular input.\n",
    "\n",
    "This is a very simple script that is incomplete -- it doesn't even handle matrix operations! -- but gives an idea of how gradients are computed.\n",
    "\n",
    "First, construct the operation as $y = f(x[, z])$ for some function $f$. Then take the derivative (here, all taken with respect to $x$ -- the extra [0] is because tf.gradients returns a list always). That happens more or less symbolically.\n",
    "\n",
    "Then, trace through the new graph created by that gradient, following along until we've reconstructed the full operation (we can print out an arbitrary operation using the getstr(tensor) function, but the gradient is what we're looking at here. Presumably, we could even actually write our own \"TensorFlow interpreter\" this way, but I'll leave that as an exercise for the reader.\n",
    "\n",
    "This shows though that the gradient maintains symbolic structure, and really computes the chain rule just as you or I might. Then, whenever the value is needed, TensorFlow will compute the actual values using its definitions of elementary operations and functions (addition, subtraction, exp, log, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 * sign(x))\n",
      "\n",
      "(1 * (2.0 * x))\n",
      "\n",
      "(1 * exp(x))\n",
      "\n",
      "((1 * exp((x)^2)) * (2.0 * x))\n",
      "\n",
      "(1 / exp((x)^2)) + (((1 * ((-x / exp((x)^2)) / exp((x)^2))) * exp((x)^2)) * (2.0 * x))\n",
      "\n",
      "((1 * exp(abs(x))) * sign(x))\n",
      "\n",
      "(1 * z)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# At least theoretically, the value of the constants are irrelevant.\n",
    "# Use your own values if you'd like.\n",
    "x = tf.constant(5.0, name=\"x\")\n",
    "z = tf.constant(2.0, name=\"z\")\n",
    "\n",
    "# Add any new variables here to print them out nicely.\n",
    "variables_with_names = [x, z]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "var_names = [t.name for t in variables_with_names]\n",
    "sess = tf.Session()\n",
    "\n",
    "def getstr(tensor):\n",
    "    unary_ops = [\"Exp\", \"Sign\", \"Abs\", \"Log\", \"Sin\",\n",
    "                 \"Sinh\", \"Cos\", \"Cosh\", \"Sqrt\", \"Tan\", \"Tanh\",\n",
    "                 \"Erf\", \"Erfc\", \"Acos\", \"Asin\", \"Atan\", \"Asinh\",\n",
    "                 \"Acosh\", \"Atanh\"]\n",
    "    unary_replace_ops = {\"Digamma\": unichr(936),\n",
    "                         \"Lgamma\": \"ln \" + unichr(915)}\n",
    "    binary_replace_ops = {\"Zeta\": unichr(950),\n",
    "                          \"Maximum\": \"max\",\n",
    "                          \"Minimum\": \"min\"}\n",
    "\n",
    "    # Already a variable we know about.\n",
    "    if tensor.name in var_names:\n",
    "        # TensorFlow takes a name \"x\", creates a unique number for\n",
    "        # this session:\n",
    "        # For example \"x_11:0\".\n",
    "        try:\n",
    "            idx = tensor.name.index(\"_\")\n",
    "            return tensor.name[0:idx]\n",
    "        except:\n",
    "            return tensor.name\n",
    "    \n",
    "    # Simple operations like Exp get dropped-in.\n",
    "    elif tensor.op.type in unary_ops:\n",
    "        return \"%s(%s)\" % (tensor.op.type.lower(),\n",
    "                           getstr(tensor.op.inputs[0]))\n",
    "    \n",
    "    # Allow for closer-to-stats notation for certain functions\n",
    "    # like the digamma.\n",
    "    elif tensor.op.type in unary_replace_ops:\n",
    "        return \"%s(%s)\" % (unary_replace_ops[tensor.op.type],\n",
    "                           getstr(tensor.op.inputs[0]))\n",
    "    \n",
    "    # Same as unary_replace_ops, but for binary operations.\n",
    "    elif tensor.op.type in binary_replace_ops:\n",
    "        return \"%s(%s, %s)\" % (\n",
    "            binary_replace_ops[tensor.op.type],\n",
    "            getstr(tensor.op.inputs[0]),\n",
    "            getstr(tensor.op.inputs[1]))\n",
    "    \n",
    "    # Handle some special-case operations\n",
    "    elif tensor.op.type == \"Square\":\n",
    "        return \"(%s)^2\" % getstr(tensor.op.inputs[0])\n",
    "    elif tensor.op.type == \"Pow\":\n",
    "        return \"(%s)^(%s)\" % (getstr(tensor.op.inputs[0]),\n",
    "                              getstr(tensor.op.inputs[1]))\n",
    "    \n",
    "    # Multiplication, addition should use the infix operators\n",
    "    # \"+\" and \"*\".\n",
    "    elif tensor.op.type == \"Mul\":\n",
    "        return \"(%s * %s)\" % (getstr(tensor.op.inputs[0]),\n",
    "                              getstr(tensor.op.inputs[1]))\n",
    "    elif tensor.op.type == \"Sub\":\n",
    "        return \"(%s - %s)\" % (getstr(tensor.op.inputs[0]),\n",
    "                              getstr(tensor.op.inputs[1]))\n",
    "    elif tensor.op.type == \"AddN\":\n",
    "        terms = [getstr(t) for t in tensor.op.inputs]\n",
    "        return \" + \".join(terms)\n",
    "    elif tensor.op.type == \"RealDiv\":\n",
    "        return \"(%s / %s)\" % (getstr(tensor.op.inputs[0]),\n",
    "                              getstr(tensor.op.inputs[1]))\n",
    "    \n",
    "    # Don't need parens around a negation.\n",
    "    elif tensor.op.type == \"Neg\":\n",
    "        return \"-%s\" % getstr(tensor.op.inputs[0])\n",
    "    \n",
    "    # Const values (other than the variables in the computation)\n",
    "    # are just constants we need to add/multiply, etc. Can\n",
    "    # get their actual value in the graph.\n",
    "    elif tensor.op.type == \"Const\":\n",
    "        return \"%s\" % sess.run(tensor)\n",
    "    \n",
    "    # The rest of these are some odd tensorflow things.\n",
    "    \n",
    "    # Fill seems to happen on one side of a binary operator without\n",
    "    # a proper value. Just enter a 1 instead.\n",
    "    elif tensor.op.type == \"Fill\":\n",
    "        return \"1\"\n",
    "    # Sum is not like add, for some reason.\n",
    "    elif tensor.op.type in [\"Reshape\", \"Sum\"]:\n",
    "        return getstr(tensor.op.inputs[0])\n",
    "    \n",
    "    # If you want to extend this, here's a default to help you\n",
    "    # know what type of operation is needed next.\n",
    "    else:\n",
    "        return tensor.op.type\n",
    "\n",
    "y = tf.abs(x)\n",
    "print(getstr(tf.gradients(y, x)[0]))\n",
    "print\n",
    "\n",
    "y = tf.square(x)\n",
    "# Should match 2 x\n",
    "print(getstr(tf.gradients(y, x)[0]))\n",
    "print\n",
    "    \n",
    "y = tf.exp(x)\n",
    "# Should match exp(x)\n",
    "print(getstr(tf.gradients(y, x)[0]))\n",
    "print\n",
    "\n",
    "y = tf.exp(tf.square(x))\n",
    "# should match 2 x exp(x^2)\n",
    "print(getstr(tf.gradients(y, x)[0]))\n",
    "print\n",
    "\n",
    "y = x / tf.exp(tf.square(x))\n",
    "print(getstr(tf.gradients(y, x)[0]))\n",
    "print\n",
    "\n",
    "y = tf.exp(tf.abs(x))\n",
    "print(getstr(tf.gradients(y, x)[0]))\n",
    "print\n",
    "\n",
    "y = x * z\n",
    "print(getstr(tf.gradients(y, x)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
